---
layout: post
comments: true
title: "LLM Evaluation Lifecycle"
excerpt: "Build robust evaluation life cycle for your LLM applications."
date:   2025-06-08 07:00:00
mathjax: false
---

The moat behind every successful AI product is the robust evaluation, or "Evals". This is what sets your product apart from all other competitors. Through doing Evals, you get a better understanding of your users' and customers' needs more than anyone else. You learn what they want from your product, identify where your product falls short, and continuously make concrete improvements. This continuous feedback loop of evaluation and improvement is the key to building a truly exceptional product.

Before diving into Evals, it's essential to know how to determine the ground truth label - the expected, correct answer. This is usually defined by one domain expert (or group of experts) as the human evaluator, defining precisely what a successful AI output looks like and ensuring alignment with their expectations. The entire LLM evaluation process can then be effectively broken down into three core stages: Analyze, Measure, and Improve.

## Analyze

The "Analyze" stage is all about understanding **what's breaking in your AI product** and **how users are actually interacting with it**. This deep dive into errors helps you pinpoint exactly where your product falls short.

Here's how to conduct effective error analysis:

1. [**Generate Your Evaluation Dataset:**](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/homeworks/hw2/generate_synthetic_queries.py)
    * Start by listing every possible realistic query your users might throw at your app.
    * Identify the key dimensions of your product (e.g., types of inputs, specific features). Then, create unique combinations (tuples) of these dimensions to generate a comprehensive set of user queries.
    * **Your Goal:** A CSV file containing a unique ID and query for each entry (e.g., `id, query`). ([sample](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/homeworks/hw2/synthetic_queries_for_analysis.csv))
    
2. [**Run Traces on Your Evaluation Dataset:**](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/scripts/bulk_test.py)
    * Feed your evaluation dataset through your AI agent and capture its responses.
    * **Your Goal:** A CSV or JSON file that includes the `id`, `query`, and the agent's `response` for each interaction. ([sample](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/homeworks/hw2/results_20250518_215844.csv))
    
3. [**Set Up Your Annotation App:**](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/annotation/annotation.py)
    * To speed up the human feedback process, build a custom interface. This app will be crucial for "open coding" (detailed individual notes) and "axial coding" (categorizing those notes).
    * You can use existing tools like Phoenix or Braintrust, or even build your own through vibe coding using frameworks like HTML/CSS + FastAPI or FastHTML + HTMX / MonsterUI. ([tutorial](https://youtu.be/qH1dZ8JLLdU?si=8aejLH8ilEHi-qoN))
    * **Your Goal:** A functional annotation application.
    
4. **Open Coding (Detailed Human Feedback):**
    * Go through each recorded trace manually. For every interaction, provide detailed human feedback and notes on what went well or wrong. Think of this as jotting down all your raw observations.
    * **Your Goal:** Your traces CSV/JSON file will now be updated with these detailed `open_coding` notes.

5. [**Axial Coding (Categorize Failure Modes):**](https://github.com/ai-evals-course/recipe-chatbot/blob/55445295232064ba857b7a174c913120b0cb36b4/homeworks/hw2/failure_mode_taxonomy.md)
    * Now, take your open coding notes and categorize them into specific "failure modes". This means grouping similar issues together (e.g., "hallucinations," "irrelevant responses," "misinterpretation of intent").
    * You can even leverage an LLM to help you generate a taxonomy of these failure modes with a well-crafted prompt.
    * **Your Goal:** Your traces CSV/JSON file will be further updated with the `axial_coding_code`, giving you clear insights into recurring problems.


## Measure

The "Measure" stage is dedicated to **quantifying the performance of your AI application**. This is where you establish clear metrics to understand how well your product is meeting its objectives.

Here's the typical process for measuring the performance of LLM applications:

1.  **Define Your Evaluation Dataset:** Begin with a fixed set of user **inputs** (e.g., 100 queries). You can start by using inputs from the "Analyze" step that represent diverse scenarios, but it's crucial to continually add new inputs encountered from real-world production use cases.
2.  **Generate Application Responses:** Run your application on this evaluation dataset to generate **responses** for each input.
3.  **Label Responses with Pass/Fail Criteria:** Based on precise, predefined definitions of success and failure (often derived from your "Analyze" phase's failure modes), **label** each generated response as either "Pass" or "Fail".
4.  **Calculate Performance Metrics:** Aggregate your labels. For instance, if 85 out of 100 responses passed and 15 failed, the application's performance for the evaluated criteria is 85%.

This systematic approach allows you to make targeted improvements to your application and then re-measure performance to objectively verify the impact of your changes.


## Improve

After the Measure step, we have a more deterministic way to check whether changes that we made to the system actually improve the system or not for a given failure mode. It is not best practice to skip the Measure step as you will most likely just eye ball responses and subjectively decide improvements when you see less errors for the given failure mode. That does not guarantee that your system has actually improved for handling the given failure mode (need to quantify the improvement).

Here are ways to Improve an LLM system

1. Set model parameters
2. Prompt engineering
3. RAG / Tools
4. Fine tuning
5. Choose a more capable model
